{
 "metadata": {
  "name": "pipeline_on_blackbox"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Try use case Blackbox "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "from sklearn import cluster\n",
      "import matplotlib.pylab as plt\n",
      "from scipy import sparse\n",
      "from IPython.parallel import Client\n",
      "%load pipeline.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import shutil, os\n",
      "from os import path\n",
      "import sqlite3\n",
      "import json\n",
      "from sklearn.externals import joblib\n",
      "from IPython.core.display import display\n",
      "import copy as cp\n",
      "from functools import partial\n",
      "import networkx as nx\n",
      "import inspect\n",
      "from sklearn import grid_search\n",
      "from collections import defaultdict\n",
      "\n",
      "from models import *\n",
      "\n",
      "## low IO api, helper function\n",
      "def overwrite_to_db(db, table, row):\n",
      "    \"\"\"\n",
      "    db: sqlite3 datafile path\n",
      "    tablename: table in the database\n",
      "    row: dict of {col:value}\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    ## name is unique id in both data and model db\n",
      "    #update_statement = \"UPDATE %s SET %s WHERE name='%s'\" % (table, ','.join(['%s=%s' % (k,v) for k,v in row.items()]), row['name'])\n",
      "    delete_statement = \"DELETE FROM %s WHERE name='%s'\" % (table, row['name'])\n",
      "    insert_statement = \"\"\"INSERT INTO %s (%s) VALUES (%s)\"\"\" % (table, \n",
      "                                                         ','.join(row.keys()), \n",
      "                                                         ','.join([\"'%s'\" % (s,) for s in row.values()]))\n",
      "\n",
      "\n",
      "    c.execute(delete_statement) ## not ideal but fast enough for prototyping\n",
      "    c.execute(insert_statement)\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    \n",
      "def query_db(db, table, columns = '*', where = None):\n",
      "    \"\"\"\n",
      "    return a list of dict (with columns as KEYS, and query results as VALUES)\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    where = ' where ' + where if where else ''\n",
      "    statement = \"SELECT %s from %s%s\" % (','.join(columns), table, where)\n",
      "    rows = c.execute(statement)\n",
      "    column_names = columns if columns != '*' else [x[0] for x in c.description]\n",
      "    results = [dict(zip(column_names, row)) for row in rows]\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    return results\n",
      "\n",
      "def write_item(meta, bulk, item_type, project_path = None):\n",
      "    \"\"\"\n",
      "    write data/model to the meta database and binary files\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## insert meta information to meta.db\n",
      "    overwrite_to_db(db = item_meta_db, table = item_meta_table, row = meta)\n",
      "    ## save data_bulk into database\n",
      "    bulk_folder = path.join(item_folder, meta['name'])\n",
      "    if path.exists(bulk_folder):\n",
      "        shutil.rmtree(bulk_folder)\n",
      "    os.mkdir(bulk_folder)\n",
      "    bulk_file = path.join(bulk_folder, 'bulk.pkl')\n",
      "    joblib.dump(bulk, bulk_file)\n",
      "    \n",
      "def read_meta_by_name(item_name, item_type, project_path = None):\n",
      "    \"\"\"\n",
      "    return meta informatino of item from the corresponding meta database\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## query the database\n",
      "    results = query_db(item_meta_db, item_meta_table, columns='*', where = 'name = \"%s\"' % item_name)\n",
      "    return results[0] if results else None\n",
      "\n",
      "def read_bulk_by_name(item_name, item_type, project_path = None):\n",
      "    \"\"\"\n",
      "    load the model/data bulk into the memory and return it\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way to bulk\n",
      "    type_folder = get_config(prefix+'folder', project_path)\n",
      "    item_file = path.join(type_folder, item_name, 'bulk.pkl')\n",
      "    item = joblib.load(item_file)\n",
      "    return item\n",
      "\n",
      "def hglue(*dfs):\n",
      "    \"\"\"horizontally stack all the df in dfs\n",
      "    example: xy_df = hglue(X, y) \n",
      "    \"\"\"\n",
      "    return pd.concat(dfs, axis = 1)\n",
      "\n",
      "def get_config(item, project_path = None):\n",
      "    project_path = project_path or '.' ## stupid tradeoff, should be improved LATER!!\n",
      "    CONFIG = {  'data_folder': path.abspath(path.join(project_path, 'data'))\n",
      "              , 'model_folder': path.abspath(path.join(project_path, 'models'))\n",
      "              , 'temp_folder': path.abspath(path.join(project_path, 'temp'))\n",
      "              , 'data_meta_db': path.abspath(path.join(project_path, 'data/meta.db'))\n",
      "              , 'data_meta_table': 'data_meta'\n",
      "              , 'data_meta_schema': \"\"\"(name text PRIMARY KEY, namespace text, input_features text, \n",
      "                                      output_features text, type integer)\"\"\"\n",
      "              , 'model_meta_db': path.abspath(path.join(project_path, 'models/meta.db'))\n",
      "              , 'model_meta_table': 'model_meta'\n",
      "              , 'model_meta_schema': '(name text PRIMARY KEY, type integer, train_data type)'\n",
      "              , 'data_signature_template': ('namespace', 'input_features', 'output_features', 'type')}\n",
      "    return CONFIG[item]\n",
      "\n",
      "class DataType(object):\n",
      "    \"\"\"data type \n",
      "    PRINCIPLE: THEY MUST BE EXCLUSIVE OF EACH\n",
      "    \"\"\"\n",
      "    UNSUPERVISED = 2 ** 0\n",
      "    BINARY_CLASSIFICATION = 2 ** 1\n",
      "    MULTI_CLASSIFICATION = 2 ** 2\n",
      "    REGRESSION = 2 ** 3\n",
      "class ModelType(object):\n",
      "    \"\"\"model type\n",
      "    which type of model can handle which set of data types \n",
      "    \"\"\"\n",
      "    BINARY_CLASSIFIER = DataType.BINARY_CLASSIFICATION\n",
      "    MULTI_CLASSIFIER = DataType.BINARY_CLASSIFICATION + DataType.MULTI_CLASSIFICATION ## or relationship\n",
      "    REGRESSOR = DataType.REGRESSION\n",
      "    ## e.g. clustering, auto_encoder, or feature_selector or dim-reductor\n",
      "    FEATURE_EXTRACTOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    ## e.g. missing value imputator\n",
      "    PREPROCESSOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    # subsampling rows\n",
      "    SUBSAMPLER = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "\n",
      "## models and data\n",
      "\n",
      "def write_project(container_path, project_name):\n",
      "    ## check if project exists - overwrite\n",
      "    project_path = path.abspath(path.join(container_path, project_name))\n",
      "    if path.exists(project_path):\n",
      "        shutil.rmtree(project_path)\n",
      "    ## create folder for project\n",
      "    os.mkdir(project_path)\n",
      "    data_folder = get_config('data_folder', project_path)\n",
      "    model_folder = get_config('model_folder', project_path)\n",
      "    temp_folder = get_config('temp_folder', project_path)\n",
      "    ## data folder\n",
      "    os.mkdir(data_folder)\n",
      "    data_meta_db = get_config('data_meta_db', project_path)\n",
      "    data_meta_table = get_config('data_meta_table', project_path)\n",
      "    data_meta_schema = get_config('data_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(data_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (data_meta_table, data_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()   \n",
      "    ## models folder\n",
      "    os.mkdir(model_folder)\n",
      "    model_meta_db = get_config('model_meta_db', project_path)\n",
      "    model_meta_table = get_config('model_meta_table', project_path)\n",
      "    model_meta_schema = get_config('model_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(model_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (model_meta_table, model_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    ## temp folder\n",
      "    os.mkdir(temp_folder)\n",
      "    return project_path\n",
      "\n",
      "def create_model_meta(model_name, model_type):\n",
      "    return {'name': model_name, \n",
      "            'type': model_type}\n",
      "\n",
      "def create_data_meta(data_name, data_namespace, input_feats, output_feats, data_type):\n",
      "    return {  'name': data_name\n",
      "            , 'namespace': data_namespace\n",
      "            , 'input_features': input_feats\n",
      "            , 'output_features': output_feats\n",
      "            , 'type': data_type}\n",
      "    \n",
      "def write_data(data_meta, data_bulk, project_path = None):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    meta = cp.deepcopy(data_meta)\n",
      "    ## JSON can only serialize built in list\n",
      "    meta['input_features'] = json.dumps([fi for fi in meta['input_features']])\n",
      "    meta['output_features'] = json.dumps([fo for fo in meta['output_features']])\n",
      "    write_item(project_path = project_path, meta = meta, bulk = data_bulk, item_type = 'data')\n",
      "    \n",
      "def write_model(model_meta, model_bulk, project_path = None):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    write_item(project_path = project_path, meta = model_meta, bulk = model_bulk, item_type = 'model')\n",
      "    \n",
      "def read_data_meta(data_name, project_path = None):\n",
      "    \"\"\"\n",
      "    return dictionary of data meta, as in the data/meta.db/data_meta table\n",
      "    \"\"\"\n",
      "    meta = read_meta_by_name(project_path = project_path, item_name = data_name, item_type = 'data')\n",
      "    meta['input_features'] = json.loads(meta['input_features'])\n",
      "    meta['output_features'] = json.loads(meta['output_features'])\n",
      "    return meta\n",
      "\n",
      "def read_model_meta(model_name, project_path = None):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    return read_meta_by_name(project_path = project_path, item_name = model_name, item_type = 'model')\n",
      "\n",
      "def read_data_bulk(data_name, project_path = None):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path = project_path, item_name = data_name, item_type = 'data')\n",
      "\n",
      "def read_model_bulk(model_name, project_path = None):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path = project_path, item_name = model_name, item_type = 'model')\n",
      "\n",
      "def trainable(model_meta, data_meta):\n",
      "    \"\"\"\n",
      "    to test if model trainable on certain data\n",
      "    RULE: model_type mactchs data_type\n",
      "    \"\"\"\n",
      "    ## compare type information\n",
      "    model_type, data_type = model_meta['type'], data_meta['type']\n",
      "    match = (model_type & data_type > 0)\n",
      "    return match\n",
      "\n",
      "def predictable(model_meta, data_meta, project_path = None):\n",
      "    \"\"\"to test if model can be used to predict on data\n",
      "    RULE: train_data's signature matches new_data's signature\n",
      "    signature of data includes (namespace, input_feats, output_feats, type)\n",
      "    \"\"\"\n",
      "    try:\n",
      "        train_meta = read_data_meta(project_path = project_path, data_name = model_meta['train_data'])\n",
      "    except Exception, e:\n",
      "        raise e\n",
      "        #raise RuntimeError('the model %s has NOT been trained on any data yet' % (model_meta['name'], ))\n",
      "    signature_template = get_config('data_signature_template')\n",
      "    train_sig = {sig:train_meta[sig] for sig in signature_template}\n",
      "    data_sig = {sig:data_meta[sig] for sig in signature_template}\n",
      "    ## rules to decide if data_sig is compatible with a model trained on data_sig\n",
      "    ## data.namespace == train.namespace\n",
      "    ## data.input_features >= train.input_features\n",
      "    ## data.output_features == train.output_features - NO NEED FOR PREDICTION AT ALL\n",
      "    ## data.type == train.type\n",
      "    namespace_match = data_sig['namespace'] == train_sig['namespace']\n",
      "    inputs_match = set(data_sig['input_features']).issuperset(set(train_sig['input_features']))\n",
      "    #outputs_match = set(data_sig['output_features']) == set(train_sig['output_features'])\n",
      "    type_match = data_sig['type'] == train_sig['type']\n",
      "    compatible = namespace_match and inputs_match and type_match\n",
      "    return compatible\n",
      "\n",
      "def transformable(model_meta, data_meta, project_path = None):\n",
      "    \"\"\"to test if model can be used to transform on data (e.g. feature extractor/selector)\n",
      "    RULE: train_data's signature matches new_data's signature\n",
      "    signature of data includes (namespace, input_feats, output_feats, type)\n",
      "    \"\"\"\n",
      "    try:\n",
      "        train_meta = read_data_meta(project_path = project_path, data_name = model_meta['train_data'])\n",
      "    except Exception, e:\n",
      "        raise e\n",
      "        #raise RuntimeError('the model %s has NOT been trained on any data yet' % (model_meta['name'], ))\n",
      "    signature_template = get_config('data_signature_template')\n",
      "    train_sig = {sig:train_meta[sig] for sig in signature_template}\n",
      "    data_sig = {sig:data_meta[sig] for sig in signature_template}\n",
      "    ## rules to decide if data_sig is compatible with a model trained on data_sig\n",
      "    ## data.namespace == train.namespace\n",
      "    ## data.input_features >= train.input_features\n",
      "    ## data.output_features == train.output_features - NO NEED FOR PREDICTION AT ALL\n",
      "    ## data.type == train.type\n",
      "    namespace_match = data_sig['namespace'] == train_sig['namespace']\n",
      "    inputs_match = set(data_sig['input_features']).issuperset(set(train_sig['input_features']))\n",
      "    #outputs_match = set(data_sig['output_features']) == set(train_sig['output_features'])\n",
      "    type_match = data_sig['type'] == train_sig['type']\n",
      "    compatible = namespace_match and inputs_match and type_match\n",
      "    return compatible\n",
      "\n",
      "def train_meta_on(model_meta, data_meta, trained_model_name):\n",
      "    \"\"\"\n",
      "    create and return trained_model_meta based on the previous model meta and data meta\n",
      "    \"\"\"\n",
      "    trained_model_meta = cp.deepcopy(model_meta)\n",
      "    train_data = data_meta['name']\n",
      "    trained_model_meta.update({'name': trained_model_name, 'train_data': train_data})\n",
      "    return trained_model_meta\n",
      "    \n",
      "def train_on(model_name, data_name, trained_model_name, project_path = None):\n",
      "    \"\"\"\n",
      "    STEPS:\n",
      "    1. load model_meta and data_meta if type DOESNT match, raise Exception\n",
      "    2. load model_bulk and data_bulk into memory\n",
      "    3. call model_bulk.fit(data_bulk)\n",
      "    4. generate the newmodel and save it by trained_model_name\n",
      "    \"\"\"\n",
      "    ## test if model is trainable on data\n",
      "    model_meta = read_model_meta(project_path = project_path, model_name = model_name)\n",
      "    data_meta = read_data_meta(project_path = project_path, data_name = data_name)\n",
      "    if not trainable(model_meta, data_meta):\n",
      "        raise RuntimeError(\"model %s is not trainable on dataset %s\" % (model_name, data_name))\n",
      "    ## load into memory\n",
      "    model_bulk = read_model_bulk(project_path = project_path, model_name = model_name)\n",
      "    data_bulk = read_data_bulk(project_path = project_path, data_name = data_name)\n",
      "    ## call model.fit(data)\n",
      "    input_feats = data_meta['input_features']\n",
      "    output_feats = data_meta['output_features']\n",
      "    if len(output_feats) == 1:\n",
      "        output_feats = output_feats[0]\n",
      "    data_input = np.asarray(data_bulk.loc[:, input_feats])\n",
      "    data_output = np.asarray(data_bulk.loc[:, output_feats])\n",
      "    model_bulk.fit(data_input, data_output)\n",
      "    ## generate new model\n",
      "    trained_model_meta = train_meta_on(model_meta, data_meta, trained_model_name)\n",
      "    write_model(project_path = project_path, model_meta = trained_model_meta, model_bulk = model_bulk)\n",
      "    \n",
      "def predict_on(model_name, data_name, predicted_data_name, project_path = None):\n",
      "    \"\"\"\n",
      "    STEP:\n",
      "    1. trace model train_data's meta\n",
      "    2. compare train_data signature with new_data signature to see if they match\n",
      "    3. signature defined as namespace/namespace1\n",
      "    \"\"\"\n",
      "    ## read meta information\n",
      "    model_meta = read_model_meta(project_path = project_path, model_name = model_name)\n",
      "    model_train_meta = read_data_meta(project_path = project_path, data_name = model_meta['train_data'])\n",
      "    data_meta = read_data_meta(project_path = project_path, data_name = data_name)\n",
      "    if not predictable(model_meta, data_meta, project_path):\n",
      "        raise RuntimeError(\"model %s cannot predict on data %s\" % (model_name, data_name))\n",
      "    ## load the data and trained model into memory\n",
      "    model_bulk = read_model_bulk(project_path = project_path, model_name = model_name)\n",
      "    data_bulk = read_data_bulk(project_path = project_path, data_name = data_name)\n",
      "    input_features = model_train_meta['input_features']\n",
      "    output_features = model_train_meta['output_features']\n",
      "    ## fit the data into the shape of model's train data\n",
      "    X = np.asarray(data_bulk.loc[:, input_features])\n",
      "    yhat = model_bulk.predict(X)\n",
      "    ## combine yhat with original data\n",
      "    yhat = pd.DataFrame(yhat, columns = output_features)\n",
      "    predicted_data_bulk = data_bulk\n",
      "    predicted_data_bulk.update(yhat, join = 'left')\n",
      "    predicted_data_meta = data_meta\n",
      "    predicted_data_meta.update({'name': predicted_data_name, 'output_features': output_features})\n",
      "    write_data(project_path = project_path, data_meta = predicted_data_meta, data_bulk = predicted_data_bulk)\n",
      "\n",
      "def transform_on(model_name, data_name, transformed_data_name, project_path = None):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    model_meta = read_model_meta(project_path = project_path, model_name = model_name)\n",
      "    model_train_meta = read_data_meta(project_path = project_path, data_name = model_meta['train_data'])\n",
      "    data_meta = read_data_meta(project_path = project_path, data_name = data_name)\n",
      "    if not transformable(model_meta, data_meta, project_path):\n",
      "        raise RuntimeError(\"model %s cannot transform on data %s\" % (model_name, data_name))\n",
      "\n",
      "    model_bulk = read_model_bulk(project_path = project_path, model_name = model_name)\n",
      "    data_bulk = read_data_bulk(project_path = project_path, data_name = data_name)\n",
      "    input_features = model_train_meta['input_features']\n",
      "    output_features = model_train_meta['output_features']\n",
      "\n",
      "    X = np.asarray(data_bulk.loc[:, input_features])\n",
      "    yhat = model_bulk.transform(X)\n",
      "    ## combine yhat with original data\n",
      "    n_ycols = yhat.shape[1]\n",
      "    transformed_features = ['%s_%i' % (model_name, i) for i in xrange(n_ycols)]\n",
      "    yhat = pd.DataFrame(yhat, columns = transformed_features)\n",
      "    predicted_data_bulk = data_bulk\n",
      "    #predicted_data_bulk.update(yhat, join = 'left')\n",
      "    #predicted_data_bulk = hglue([data_bulk, yhat])\n",
      "    ## excluding original input features\n",
      "    predicted_data_bulk = hglue(yhat, data_bulk.loc[:, output_features])\n",
      "    predicted_data_meta = data_meta\n",
      "    predicted_data_meta.update({'name': transformed_data_name, 'input_features': transformed_features})\n",
      "    write_data(project_path = project_path, data_meta = predicted_data_meta, data_bulk = predicted_data_bulk)\n",
      "\n",
      "def score_on(target_data_name, predicted_data_name, score_fn, project_path = None):\n",
      "    \"\"\"\n",
      "    the target_data and predicted_data should have the common set of output feature\n",
      "    the method will compare the output features and output and apply score_fn on them\n",
      "    examples of score_fn include: (1) sklearn.metrics.XX\n",
      "    TODO: consider the type of data to test if the certain score_fn is appliable to them\n",
      "    \"\"\"\n",
      "    ## read meta\n",
      "    target_meta = read_data_meta(project_path = project_path, data_name = target_data_name)\n",
      "    predicted_meta = read_data_meta(project_path = project_path, data_name = predicted_data_name)\n",
      "    assert target_meta['output_features'] == predicted_meta['output_features'] \n",
      "    output_features = target_meta['output_features']\n",
      "    ## read bulk\n",
      "    y = np.asarray(read_data_bulk(project_path = project_path, data_name = target_data_name).loc[:, output_features])\n",
      "    yhat = np.asarray(read_data_bulk(project_path = project_path, data_name = predicted_data_name).loc[:, output_features])\n",
      "    ## apply scorefn \n",
      "    return score_fn(y, yhat)\n",
      "\n",
      "def set_params_on_model(model_name, model_params, project_path):\n",
      "    \"\"\"\n",
      "    params: {param_name: param_value}\n",
      "    \"\"\"\n",
      "    ## find model meta and bulk\n",
      "    model_meta = read_model_meta(model_name, project_path)\n",
      "    model_bulk = read_model_bulk(model_name, project_path)\n",
      "    ## set model params on bulk\n",
      "    ## model meta will not be changed\n",
      "    model_bulk.set_params(**model_params)\n",
      "    ## save it back\n",
      "    write_model(model_meta, model_bulk, project_path)\n",
      "\n",
      "## pipeline \n",
      "\n",
      "def make_pipeline(pipeline_name):\n",
      "    pipeline = nx.DiGraph()\n",
      "    pipeline.name = pipeline_name\n",
      "    #pipeline.max_id = 0 ## infinite source of id\n",
      "    return pipeline\n",
      "\n",
      "def make_pipe(fn, pipeline, pipe_name):\n",
      "    #pipe_id = pipeline.max_id\n",
      "    #pipeline.max_id += 1 \n",
      "    arg_names = inspect.getargspec(fn).args\n",
      "    pipe_config = {'name': pipe_name, 'fn': fn, 'args': arg_names, \n",
      "                    'bindings': {k:None for k in arg_names}}\n",
      "    pipeline.add_node(pipe_name, pipe_config)\n",
      "    return pipe_name\n",
      "\n",
      "def connect_pipes(from_id, out_arg, to_id, in_arg, pipeline):\n",
      "    pipeline.add_edge(from_id, to_id)\n",
      "    from_node = pipeline.node[from_id]\n",
      "    to_node = pipeline.node[to_id]\n",
      "    #assert from_node['bindings'][out_arg] is None\n",
      "    #assert to_node['bindings'][in_arg] is None\n",
      "    #common_item = '%s_(%s_TO_%s)' % (pipeline.name, from_node['name'], to_node['name'])\n",
      "    ## because one output might flow into multiple inputs of different nodes\n",
      "    common_item = '%s_(%s_OUT)' % (pipeline.name, from_node['name'])\n",
      "    from_node['bindings'][out_arg] = common_item\n",
      "    to_node['bindings'][in_arg] = common_item\n",
      "    return pipeline\n",
      "\n",
      "def bind_args(args, values, pipe_id, pipeline):\n",
      "    pipe_node = pipeline.node[pipe_id]\n",
      "    for arg, value in zip(args, values):\n",
      "        assert pipe_node['bindings'][arg] is None\n",
      "        pipe_node['bindings'][arg] = value\n",
      "        \n",
      "def run_pipeline(pipeline):\n",
      "    for pipe_id in nx.topological_sort(pipeline):\n",
      "        pipe_node = pipeline.node[pipe_id]\n",
      "        fn = pipe_node['fn']\n",
      "        bindings = pipe_node['bindings']\n",
      "        fn(**bindings)\n",
      "\n",
      "def runnable_pipeline(pipeline):\n",
      "    def _pipeline():\n",
      "        run_pipeline(pipeline)\n",
      "    return _pipeline\n",
      "\n",
      "def inspect_pipeline(pipeline):\n",
      "    figure(figsize=(12, 12))\n",
      "    layout = nx.graphviz_layout(pipeline)\n",
      "    nx.draw(pipeline, pos = layout)\n",
      "\n",
      "## parameter optimization\n",
      "\n",
      "def random_param_optimize(models2params, n_iter, fn0, scorefn0, project_path):\n",
      "    \"\"\"\n",
      "    models2params: {modelname: {paramname: paramvalue, ...}, ...}\n",
      "    n_iter: number of tries\n",
      "    fn = zero_arg function that runs and uses models or datafiles, \n",
      "        e.g., partial(train_on), runnable_pipeline instance and etc.\n",
      "    scorefn0: zero_arg function that tells how good the performance of each run is, \n",
      "        e.g., partial(score_on, metric.XXX) \n",
      "    RETURN: [(setting1, score1), ...], setting = {(model_name, param_name): param_value}\n",
      "    \"\"\"\n",
      "    ## group parameters\n",
      "    flat_params = {}\n",
      "    for model_name, params in models2params.items():\n",
      "        for param_name,  param_values in params.items():\n",
      "            flat_param_name = (model_name, param_name)\n",
      "            flat_params[flat_param_name] = param_values\n",
      "    ## sampling parameter space\n",
      "    param_sampler = grid_search.ParameterSampler(flat_params, n_iter)\n",
      "\n",
      "    results = []\n",
      "    ## iterate each param setting\n",
      "    for setting in param_sampler:\n",
      "        ## get params for each model\n",
      "        models2settings = defaultdict(dict) \n",
      "        for (model_name, param_name), param_value in setting.items():\n",
      "            models2settings[model_name][param_name] = param_value\n",
      "        ## set params on each model\n",
      "        for model_name, params in models2settings.items():\n",
      "            set_params_on_model(model_name, params, project_path)\n",
      "        ## run fn \n",
      "        fn0() \n",
      "        ## record score \n",
      "        score = scorefn0()\n",
      "        results.append((setting, score)) \n",
      "    ## sort the results based on score:\n",
      "    results = sorted(results, key = lambda (setting, score): score, reverse = True)\n",
      "    return results "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "test script"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## clean up all the existing folder\n",
      "project_name = 'prototype_pipe'\n",
      "container_path = 'data'\n",
      "\n",
      "project_path = write_project(container_path, project_name)\n",
      "\n",
      "# what is \"!tree data\" in MAC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##load blackbox data\n",
      "bb_data, bb_target = cPickle.load(open('/Users/grace/workspace/automl/data/blackbox.pkl', 'rb'))\n",
      "print bb_data.shape, bb_target.shape\n",
      "print np.unique(bb_target), bb_target.dtype ## classificaiton \n",
      "print type(bb_data), bb_data.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1000, 1875) (1000,)\n",
        "[1 2 3 4 5 6 7 8 9] int64\n",
        "<type 'numpy.ndarray'> 1875\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##create, persist meta data\n",
      "bb_meta = {  'name': 'bb' \n",
      "             , 'namespace': 'bb'\n",
      "             , 'input_features': ['%s_%i' % (\"feat\", i) for i in xrange(bb_data.shape[1])]\n",
      "             , 'output_features': [\"classes\"]\n",
      "             , 'type': DataType.MULTI_CLASSIFICATION}\n",
      "X = pd.DataFrame(bb_data, columns = ['%s_%i' % (\"feat\", i) for i in xrange(bb_data.shape[1])])\n",
      "y = pd.DataFrame(bb_target, columns = ['classes'])\n",
      "print type(X), type(y)\n",
      "print X.columns\n",
      "bb_bulk = hglue(X, y)\n",
      "write_data(bb_meta, bb_bulk, project_path)\n",
      "print bb_bulk.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n",
        "Index([feat_0, feat_1, feat_2, ..., feat_1872, feat_1873, feat_1874], dtype=object)\n",
        "Index([feat_0, feat_1, feat_2, ..., feat_1873, feat_1874, classes], dtype=object)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##create model template\n",
      "from sklearn import ensemble\n",
      "sgd_template = linear_model.SGDClassifier()\n",
      "pca_template = decomposition.PCA(n_components= 200)\n",
      "pac_template = linear_model.PassiveAggressiveClassifier()\n",
      "rf_template = ensemble.RandomForestClassifier(n_estimators=500)\n",
      "kmeans_template = cluster.KMeans()\n",
      "\n",
      "\n",
      "sgd_meta = create_model_meta(model_name = 'sgd_template', model_type = ModelType.MULTI_CLASSIFIER)\n",
      "pca_meta = create_model_meta(model_name = 'pca_template', model_type = ModelType.FEATURE_EXTRACTOR)\n",
      "pac_meta = create_model_meta(model_name = 'pac_template', model_type = ModelType.MULTI_CLASSIFIER)\n",
      "rf_meta = create_model_meta(model_name = 'rf_template', model_type = ModelType.MULTI_CLASSIFIER)\n",
      "kmeans_meta = create_model_meta(model_name = 'kmeans_template', model_type = ModelType.FEATURE_EXTRACTOR)\n",
      "\n",
      "write_model(sgd_meta, sgd_template, project_path)\n",
      "write_model(pca_meta, pca_template, project_path)\n",
      "write_model(pac_meta, pac_template, project_path)\n",
      "write_model(rf_meta, rf_template, project_path)\n",
      "write_model(kmeans_meta, kmeans_template, project_path)\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#def train_on(model_name, data_name, trained_model_name, project_path = None)\n",
      "#def predict_on(model_name, data_name, predicted_data_name, project_path = None)\n",
      "#def transform_on(model_name, data_name, transformed_data_name, project_path = None)\n",
      "pca_sgd_pipeline = make_pipeline('pca_sgd_pipeline')\n",
      "\n",
      "train_pca_pipe = make_pipe(train_on, pca_sgd_pipeline, 'train_pca_pipe')\n",
      "transform_pca_pipe_train = make_pipe(transform_on, pca_sgd_pipeline, 'transform_pca_pipe_train')\n",
      "train_sgd_pipe = make_pipe(train_on, pca_sgd_pipeline, 'train_sgd_pipe')\n",
      "transform_pca_pipe_test = make_pipe(transform_on, pca_sgd_pipeline, 'transform_pca_pipe_test')\n",
      "predict_sgd_pipe = make_pipe(predict_on, pca_sgd_pipeline, 'predict_sgd_pipe')\n",
      "\n",
      "\n",
      "\n",
      "connect_pipes(from_id = train_pca_pipe, out_arg = 'trained_model_name', \n",
      "              to_id = transform_pca_pipe_train, in_arg = 'model_name', \n",
      "              pipeline = pca_sgd_pipeline)\n",
      "connect_pipes(from_id = transform_pca_pipe_train, out_arg = 'transformed_data_name', \n",
      "              to_id = train_sgd_pipe, in_arg = 'data_name',\n",
      "              pipeline = pca_sgd_pipeline)\n",
      "connect_pipes(from_id = train_sgd_pipe, out_arg = 'trained_model_name',\n",
      "              to_id = predict_sgd_pipe, in_arg = 'model_name',\n",
      "              pipeline = pca_sgd_pipeline)\n",
      "connect_pipes(from_id = train_pca_pipe, out_arg = 'trained_model_name', \n",
      "              to_id = transform_pca_pipe_test, in_arg = 'model_name', \n",
      "              pipeline = pca_sgd_pipeline)\n",
      "connect_pipes(from_id = transform_pca_pipe_test, out_arg = 'transformed_data_name', \n",
      "              to_id = predict_sgd_pipe, in_arg = 'data_name', \n",
      "              pipeline = pca_sgd_pipeline)\n",
      "\n",
      "bind_args(args = ['model_name', 'data_name', 'project_path'], \n",
      "          values = ['pca_template','bb', project_path], \n",
      "          pipe_id = train_pca_pipe, pipeline = pca_sgd_pipeline)\n",
      "bind_args(args = ['data_name', 'project_path'],\n",
      "          values = ['bb', project_path],\n",
      "          pipe_id = transform_pca_pipe_train, pipeline = pca_sgd_pipeline)\n",
      "bind_args(args =['model_name', 'project_path'],\n",
      "          values = ['sgd_template', project_path],\n",
      "          pipe_id = train_sgd_pipe, pipeline = pca_sgd_pipeline)\n",
      "bind_args(args =['data_name', 'project_path'],\n",
      "          values = ['bb', project_path],\n",
      "          pipe_id = transform_pca_pipe_test, pipeline = pca_sgd_pipeline)\n",
      "bind_args(args = ['predicted_data_name', 'project_path'],\n",
      "          values = ['pca_sgd_FOR_bb', project_path],\n",
      "          pipe_id = predict_sgd_pipe, pipeline = pca_sgd_pipeline)\n",
      "\n",
      "run_pipeline(pca_sgd_pipeline)\n",
      "#run_pipeline()\n",
      "run_pca_sgd_pipeline = runnable_pipeline(pca_sgd_pipeline)\n",
      "run_pca_sgd_pipeline()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score_on('bb', 'pca_sgd_FOR_bb', metrics.accuracy_score, project_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.316"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pca_params = \n",
      "#set_params_on_model(pca_template, , project_name)\n",
      "## test parameter search\n",
      "## 1 test train_on\n",
      "\n",
      "models2params = {'sgd_template': {'alpha': np.logspace(-1, 0, 2), 'loss': ['hinge']}}\n",
      "random_param_optimize(models2params = models2params, \n",
      "                      n_iter = 3, \n",
      "                      fn0 = run_pca_sgd_pipeline, \n",
      "                      scorefn0 = partial(score_on, target_data_name = 'bb', \n",
      "                                         predicted_data_name = 'pca_sgd_FOR_bb', \n",
      "                                         score_fn = metrics.accuracy_score, \n",
      "                                         project_path = project_path), \n",
      "                      project_path = project_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[({('sgd_template', 'alpha'): 0.10000000000000001,\n",
        "   ('sgd_template', 'loss'): 'hinge'},\n",
        "  0.23899999999999999),\n",
        " ({('sgd_template', 'alpha'): 0.10000000000000001,\n",
        "   ('sgd_template', 'loss'): 'hinge'},\n",
        "  0.23899999999999999),\n",
        " ({('sgd_template', 'alpha'): 0.10000000000000001,\n",
        "   ('sgd_template', 'loss'): 'hinge'},\n",
        "  0.23899999999999999)]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}